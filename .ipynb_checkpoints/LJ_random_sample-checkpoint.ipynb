{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cathedral-captain",
   "metadata": {},
   "source": [
    "# Create random sample (to give to Michael)\n",
    "Here's how we created the random sample (i.e. various halos in LJ, some are FGs, most are not). It is built to include three narrow mass bins (same as the default bins in `make_masks()` and in each mass bin, there should be as many halos (random) as there are total number of fossils in that halo (as calculated from the `fg_forest`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-block",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "equipped-partnership",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyfftw not available, using numpy fft\n"
     ]
    }
   ],
   "source": [
    "import haccytrees.mergertrees\n",
    "import h5py\n",
    "import numpy as np\n",
    "import numba\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "import pandas as pd\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "import astropy.units as u\n",
    "from itertools import groupby\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "%load_ext line_profiler\n",
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport help_func_haccytrees\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    'font.size': 13,\n",
    "    \"figure.figsize\": (5.25, 3.5),#(6.25, 4.25), #(6.25, 3.75)\n",
    "    \"patch.linewidth\": 1\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-bunny",
   "metadata": {},
   "source": [
    "#### What is `data`?\n",
    "`data` is a dictionary containing only the halo information we are interested in (i.e. just at $z=0$). Use `data.keys()` to see what's inside. You can treat `data` largely like you treated `forest`: i.e. to get a `key` for a specific subset of halos, use `data['key_name'][halo_idx]`.\n",
    "On the fossil groups side, the equivalent of `data` is `fg_catalog`, which consists only of $z=0$ information for halos we have previously identified as fossil groups. We create `fg_catalog` from `fg_forest.hdf5` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "southwest-spain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.1 s, sys: 19.7 s, total: 50.8 s\n",
      "Wall time: 50.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with h5py.File(\"/data/a/cpac/mbuehlmann/LastJourney/forest/z0_catalog.hdf5\", \"r\") as f:\n",
    "    data = {\n",
    "        k: d[:] for k, d in f.items()\n",
    "    }\n",
    "s = np.argsort(data['tree_node_index'])\n",
    "data = {k: data[k][s] for k in data.keys()} # all halos at z=0 above a certain mass threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "future-regular",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['delta+1_rsoft0.0', 'delta+1_rsoft1.0', 'delta+1_rsoft10.0', 'delta+1_rsoft2.0', 'delta+1_rsoft4.0', 'filenum', 'index', 'sig_rsoft_0.0', 'sig_rsoft_1.0', 'sig_rsoft_10.0', 'sig_rsoft_2.0', 'sig_rsoft_4.0', 'sod_halo_cdelta', 'sod_halo_cdelta_accum', 'sod_halo_cdelta_peak', 'tree_node_index', 'tree_node_mass', 'x', 'xoff_com', 'xoff_fof', 'xoff_sod', 'y', 'z'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-editing",
   "metadata": {},
   "source": [
    "### Now create `fg_catalog` (similar to `data`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "determined-sarah",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.4 s, sys: 33.5 s, total: 58.8 s\n",
      "Wall time: 58.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fg_forest_lm5, fg_progenitor_array_lm5 = haccytrees.mergertrees.read_forest(\n",
    "    \"/data/a/cpac/mbuehlmann/LastJourney/forest/fg_forest.hdf5\",\n",
    "    'LastJourney',\n",
    "    mass_threshold=5e11\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "billion-trash",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.36 s, sys: 112 ms, total: 2.47 s\n",
      "Wall time: 2.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mask_z0 = fg_forest_lm5['snapnum'] == 100\n",
    "fg_catalog_lm5 = {k: fg_forest_lm5[k][mask_z0] for k in fg_forest_lm5.keys()} # forest data at z=0 # Not sure why this different from `assign_fgs()`?\n",
    "s = np.argsort(fg_catalog_lm5['tree_node_index']) # How is this being sorted exactly?\n",
    "fg_catalog_lm5 = {k: fg_catalog_lm5[k][s] for k in fg_catalog_lm5.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "indie-princess",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['branch_size', 'desc_node_index', 'fof_halo_center_x', 'fof_halo_center_y', 'fof_halo_center_z', 'fof_halo_count', 'fof_halo_mass', 'fof_halo_tag', 'snapnum', 'sod_halo_cdelta', 'sod_halo_cdelta_accum', 'sod_halo_cdelta_error', 'sod_halo_cdelta_peak', 'sod_halo_count', 'sod_halo_mass', 'sod_halo_radius', 'tree_node_index', 'tree_node_mass', 'xoff_com', 'xoff_fof', 'xoff_sod', 'scale_factor', 'descendant_idx', 'progenitor_count', 'progenitor_offset', 'halo_index'])\n"
     ]
    }
   ],
   "source": [
    "print(fg_catalog_lm5.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "spread-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create an index of fossil groups\n",
    "@numba.jit(nopython=True)\n",
    "def assign_fgs(full_tn_index, fg_tn_index):\n",
    "    n_full = len(full_tn_index)\n",
    "    n_fg = len(fg_tn_index)\n",
    "    c_full_idx = 0\n",
    "    c_fg_idx = 0\n",
    "    fg_index = np.empty(n_fg, dtype=np.int64)\n",
    "    fg_index[:] = -1\n",
    "    while c_full_idx < n_full and c_fg_idx < n_fg:\n",
    "        if full_tn_index[c_full_idx] == fg_tn_index[c_fg_idx]:\n",
    "            fg_index[c_fg_idx] = c_full_idx\n",
    "            c_fg_idx += 1\n",
    "        else:\n",
    "            c_full_idx += 1\n",
    "    return fg_index\n",
    "fg_idx = assign_fgs(data['tree_node_index'], fg_catalog_lm5['tree_node_index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-hollywood",
   "metadata": {},
   "source": [
    "### Create random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "other-future",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([2247382487065960618, 2247442453399346018, 2247372887814052310, ...,\n",
      "       2247418367222766344, 2247301406673370823, 2247517276024623655]), array([2247530736452122641, 2247559078941299220, 2247344961936722907, ...,\n",
      "       2247461042017825579, 2247530178106368471, 2247523727065495203]), array([2247311203493760669, 2247422851168633196, 2247567067580466021, ...,\n",
      "       2247442002427798725, 2247475060791066866, 2247448238720296175])]\n",
      "[array([1.0062518e+13, 1.0187519e+13, 1.0630454e+13, ..., 1.0239149e+13,\n",
      "       1.1005454e+13, 1.1051650e+13], dtype=float32), array([2.0652212e+13, 2.0548950e+13, 2.2380475e+13, ..., 2.0008189e+13,\n",
      "       2.2029932e+13, 2.1972866e+13], dtype=float32), array([4.2149534e+13, 4.4440299e+13, 4.0089745e+13, ..., 4.0182137e+13,\n",
      "       4.2462035e+13, 4.2054424e+13], dtype=float32)]\n",
      "[array([156976971,  39208088,  58070978, ..., 125788208,  57588077,\n",
      "        78619593]), array([214009278, 118449372, 143200851, ..., 229440714, 222748617,\n",
      "       143481611]), array([132018503, 166648757, 218232817, ...,  44561771,  12474962,\n",
      "        99995505])]\n",
      "[array([122, 204, 118, ..., 183,   9, 305], dtype=uint16), array([304, 373,  50, ..., 213, 297, 322], dtype=uint16), array([ 18, 184, 362, ..., 201, 273, 217], dtype=uint16)]\n"
     ]
    }
   ],
   "source": [
    "tn_idx = [] # tree node index\n",
    "mass = []\n",
    "idx = []\n",
    "file = []\n",
    "for this_bin in [[1e13, 10**13.05], [10**13.3, 10**13.35], [10**13.6, 10**13.65]]:\n",
    "    bin_mask = (data['tree_node_mass'] > this_bin[0]) & (data['tree_node_mass'] < this_bin[1])\n",
    "    bin_mask_idx, = np.nonzero(bin_mask)\n",
    "    n_fgs = len(fg_idx[(fg_catalog_lm5['tree_node_mass'] > this_bin[0]) & (fg_catalog_lm5['tree_node_mass'] < this_bin[1])])\n",
    "    random_sample_mask = np.random.choice(bin_mask_idx, n_fgs, replace = False)\n",
    "    \n",
    "    tn_idx.append(data['tree_node_index'][random_sample_mask])\n",
    "    mass.append(data['tree_node_mass'][random_sample_mask])\n",
    "    idx.append(data['index'][random_sample_mask])\n",
    "    file.append(data['filenum'][random_sample_mask])\n",
    "    \n",
    "create_file = False\n",
    "if create_file:\n",
    "    with h5py.File('random_sample.h5', 'w') as f:\n",
    "        f.create_dataset('tree_node_index', data=np.concatenate(tn_idx))\n",
    "        f.create_dataset('tree_node_mass', data=np.concatenate(mass))\n",
    "        f.create_dataset('index', data=np.concatenate(idx))\n",
    "        f.create_dataset('file', data=np.concatenate(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "trying-baking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['file', 'index', 'tree_node_index', 'tree_node_mass']>\n",
      "307993\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('random_sample.h5', 'r') as hf: # 'r' = read\n",
    "    print(hf.keys())\n",
    "    print(len(hf['tree_node_index']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fossil_groups_env",
   "language": "python",
   "name": "fossil_groups_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
