{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "detected-corner",
   "metadata": {},
   "source": [
    "# Create random sample (to give to Michael)\n",
    "Here's how we created the random sample (i.e. various halos in LJ, some are FGs, most are not). It is built to include three narrow mass bins (same as the default bins in `make_masks()` and in each mass bin, there should be as many halos (random) as there are total number of fossils in that halo (as calculated from the `fg_forest`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-sentence",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "closing-polymer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyfftw not available, using numpy fft\n"
     ]
    }
   ],
   "source": [
    "import haccytrees.mergertrees\n",
    "import h5py\n",
    "import numpy as np\n",
    "import numba\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "import pandas as pd\n",
    "from astropy.cosmology import FlatLambdaCDM\n",
    "import astropy.units as u\n",
    "from itertools import groupby\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "%load_ext line_profiler\n",
    "%reload_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport help_func_haccytrees\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    'font.size': 13,\n",
    "    \"figure.figsize\": (5.25, 3.5),#(6.25, 4.25), #(6.25, 3.75)\n",
    "    \"patch.linewidth\": 1\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-acrobat",
   "metadata": {},
   "source": [
    "#### What is `data`?\n",
    "`data` is a dictionary containing only the halo information we are interested in (i.e. just at $z=0$). Use `data.keys()` to see what's inside. You can treat `data` largely like you treated `forest`: i.e. to get a `key` for a specific subset of halos, use `data['key_name'][halo_idx]`.\n",
    "On the fossil groups side, the equivalent of `data` is `fg_catalog`, which consists only of $z=0$ information for halos we have previously identified as fossil groups. We create `fg_catalog` from `fg_forest.hdf5` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "generic-allah",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31.1 s, sys: 19.7 s, total: 50.8 s\n",
      "Wall time: 50.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with h5py.File(\"/data/a/cpac/mbuehlmann/LastJourney/forest/z0_catalog.hdf5\", \"r\") as f:\n",
    "    data = {\n",
    "        k: d[:] for k, d in f.items()\n",
    "    }\n",
    "s = np.argsort(data['tree_node_index'])\n",
    "data = {k: data[k][s] for k in data.keys()} # all halos at z=0 above a certain mass threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "medium-insertion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['delta+1_rsoft0.0', 'delta+1_rsoft1.0', 'delta+1_rsoft10.0', 'delta+1_rsoft2.0', 'delta+1_rsoft4.0', 'filenum', 'index', 'sig_rsoft_0.0', 'sig_rsoft_1.0', 'sig_rsoft_10.0', 'sig_rsoft_2.0', 'sig_rsoft_4.0', 'sod_halo_cdelta', 'sod_halo_cdelta_accum', 'sod_halo_cdelta_peak', 'tree_node_index', 'tree_node_mass', 'x', 'xoff_com', 'xoff_fof', 'xoff_sod', 'y', 'z'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-sport",
   "metadata": {},
   "source": [
    "### Now create `fg_catalog` (similar to `data`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "regulated-celebration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.4 s, sys: 33.5 s, total: 58.8 s\n",
      "Wall time: 58.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fg_forest_lm5, fg_progenitor_array_lm5 = haccytrees.mergertrees.read_forest(\n",
    "    \"/data/a/cpac/mbuehlmann/LastJourney/forest/fg_forest.hdf5\",\n",
    "    'LastJourney',\n",
    "    mass_threshold=5e11\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hidden-fluid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.36 s, sys: 112 ms, total: 2.47 s\n",
      "Wall time: 2.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mask_z0 = fg_forest_lm5['snapnum'] == 100\n",
    "fg_catalog_lm5 = {k: fg_forest_lm5[k][mask_z0] for k in fg_forest_lm5.keys()} # forest data at z=0 # Not sure why this different from `assign_fgs()`?\n",
    "s = np.argsort(fg_catalog_lm5['tree_node_index']) # How is this being sorted exactly?\n",
    "fg_catalog_lm5 = {k: fg_catalog_lm5[k][s] for k in fg_catalog_lm5.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "centered-grill",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['branch_size', 'desc_node_index', 'fof_halo_center_x', 'fof_halo_center_y', 'fof_halo_center_z', 'fof_halo_count', 'fof_halo_mass', 'fof_halo_tag', 'snapnum', 'sod_halo_cdelta', 'sod_halo_cdelta_accum', 'sod_halo_cdelta_error', 'sod_halo_cdelta_peak', 'sod_halo_count', 'sod_halo_mass', 'sod_halo_radius', 'tree_node_index', 'tree_node_mass', 'xoff_com', 'xoff_fof', 'xoff_sod', 'scale_factor', 'descendant_idx', 'progenitor_count', 'progenitor_offset', 'halo_index'])\n"
     ]
    }
   ],
   "source": [
    "print(fg_catalog_lm5.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "comic-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create an index of fossil groups\n",
    "@numba.jit(nopython=True)\n",
    "def assign_fgs(full_tn_index, fg_tn_index):\n",
    "    n_full = len(full_tn_index)\n",
    "    n_fg = len(fg_tn_index)\n",
    "    c_full_idx = 0\n",
    "    c_fg_idx = 0\n",
    "    fg_index = np.empty(n_fg, dtype=np.int64)\n",
    "    fg_index[:] = -1\n",
    "    while c_full_idx < n_full and c_fg_idx < n_fg:\n",
    "        if full_tn_index[c_full_idx] == fg_tn_index[c_fg_idx]:\n",
    "            fg_index[c_fg_idx] = c_full_idx\n",
    "            c_fg_idx += 1\n",
    "        else:\n",
    "            c_full_idx += 1\n",
    "    return fg_index\n",
    "fg_idx = assign_fgs(data['tree_node_index'], fg_catalog_lm5['tree_node_index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-observation",
   "metadata": {},
   "source": [
    "### Create random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "seven-singing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([2247382487065960618, 2247442453399346018, 2247372887814052310, ...,\n",
      "       2247418367222766344, 2247301406673370823, 2247517276024623655]), array([2247530736452122641, 2247559078941299220, 2247344961936722907, ...,\n",
      "       2247461042017825579, 2247530178106368471, 2247523727065495203]), array([2247311203493760669, 2247422851168633196, 2247567067580466021, ...,\n",
      "       2247442002427798725, 2247475060791066866, 2247448238720296175])]\n",
      "[array([1.0062518e+13, 1.0187519e+13, 1.0630454e+13, ..., 1.0239149e+13,\n",
      "       1.1005454e+13, 1.1051650e+13], dtype=float32), array([2.0652212e+13, 2.0548950e+13, 2.2380475e+13, ..., 2.0008189e+13,\n",
      "       2.2029932e+13, 2.1972866e+13], dtype=float32), array([4.2149534e+13, 4.4440299e+13, 4.0089745e+13, ..., 4.0182137e+13,\n",
      "       4.2462035e+13, 4.2054424e+13], dtype=float32)]\n",
      "[array([156976971,  39208088,  58070978, ..., 125788208,  57588077,\n",
      "        78619593]), array([214009278, 118449372, 143200851, ..., 229440714, 222748617,\n",
      "       143481611]), array([132018503, 166648757, 218232817, ...,  44561771,  12474962,\n",
      "        99995505])]\n",
      "[array([122, 204, 118, ..., 183,   9, 305], dtype=uint16), array([304, 373,  50, ..., 213, 297, 322], dtype=uint16), array([ 18, 184, 362, ..., 201, 273, 217], dtype=uint16)]\n"
     ]
    }
   ],
   "source": [
    "tn_idx = [] # tree node index\n",
    "mass = []\n",
    "idx = []\n",
    "file = []\n",
    "for this_bin in [[1e13, 10**13.05], [10**13.3, 10**13.35], [10**13.6, 10**13.65]]:\n",
    "    bin_mask = (data['tree_node_mass'] > this_bin[0]) & (data['tree_node_mass'] < this_bin[1])\n",
    "    bin_mask_idx, = np.nonzero(bin_mask)\n",
    "    n_fgs = len(fg_idx[(fg_catalog_lm5['tree_node_mass'] > this_bin[0]) & (fg_catalog_lm5['tree_node_mass'] < this_bin[1])])\n",
    "    random_sample_mask = np.random.choice(bin_mask_idx, n_fgs, replace = False)\n",
    "    \n",
    "    tn_idx.append(data['tree_node_index'][random_sample_mask])\n",
    "    mass.append(data['tree_node_mass'][random_sample_mask])\n",
    "    idx.append(data['index'][random_sample_mask])\n",
    "    file.append(data['filenum'][random_sample_mask])\n",
    "    \n",
    "create_file = False\n",
    "if create_file:\n",
    "    with h5py.File('random_sample.h5', 'w') as f:\n",
    "        f.create_dataset('tree_node_index', data=np.concatenate(tn_idx))\n",
    "        f.create_dataset('tree_node_mass', data=np.concatenate(mass))\n",
    "        f.create_dataset('index', data=np.concatenate(idx))\n",
    "        f.create_dataset('file', data=np.concatenate(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-missouri",
   "metadata": {},
   "source": [
    "### Reload random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "light-redhead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['file', 'index', 'tree_node_index', 'tree_node_mass']>\n",
      "307993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "307993"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with h5py.File('random_sample.h5', 'r') as hf: # 'r' = read\n",
    "    print(hf.keys())\n",
    "    print(len(hf['tree_node_index']))\n",
    "    bin1 = hf['tree_node_index'][:269358]\n",
    "    bin2 = hf['tree_node_index'][269358: 269358 + 36181]\n",
    "    bin3 = hf['tree_node_index'][269358 + 36181: ]\n",
    "len(bin1) + len(bin2) + len(bin3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-highlight",
   "metadata": {},
   "source": [
    "### Check statistics of the random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "roman-april",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.77 s, sys: 8.62 s, total: 15.4 s\n",
      "Wall time: 15.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "forest, progenitor_array = haccytrees.mergertrees.read_forest(\n",
    "    '/data/a/cpac/mbuehlmann/LastJourney/forest/target_forest_aurora.hdf5',\n",
    "    'LastJourney', nchunks=1, chunknum=0, #mass_threshold = 2.7*10**11,\n",
    "    include_fields = [\"tree_node_mass\", \"snapnum\", \"fof_halo_tag\", \"sod_halo_cdelta\", \"fof_halo_center_x\", \"fof_halo_center_y\", \"fof_halo_center_z\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "current-enough",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307993\n"
     ]
    }
   ],
   "source": [
    "# Make masks\n",
    "halo_masks = help_func_haccytrees.make_masks(forest) # Forest should already only contain values in these bins\n",
    "print(len(halo_masks[0][halo_masks[0]]) + len(halo_masks[1][halo_masks[1]]) + len(halo_masks[2][halo_masks[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "specific-client",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual min:  10000000000000\n",
      "this min:    10000018000000.0\n",
      "actual max:  11220184543019.652\n",
      "this max:    11220130000000.0\n"
     ]
    }
   ],
   "source": [
    "print(\"actual min: \", 10**13)\n",
    "print(\"this min:   \", np.min(forest['tree_node_mass'][np.nonzero(halo_masks[0])]))\n",
    "print(\"actual max: \", 10**13.05)\n",
    "print(\"this max:   \", np.max(forest['tree_node_mass'][np.nonzero(halo_masks[0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "duplicate-doubt",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "bin  0\n",
      "269358  halos\n",
      "33569  fossils\n",
      "4171  qhs\n",
      "15581  merger rich halos\n",
      "1\n",
      "bin  1\n",
      "36181  halos\n",
      "1364  fossils\n",
      "9  qhs\n",
      "2063  merger rich halos\n",
      "2\n",
      "bin  2\n",
      "2454  halos\n",
      "13  fossils\n",
      "0  qhs\n",
      "126  merger rich halos\n",
      "CPU times: user 3.04 s, sys: 677 ms, total: 3.72 s\n",
      "Wall time: 2.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "absolute_threshold = True\n",
    "threshold = 5e11\n",
    "z_thresh = 1\n",
    "norm_tf = False\n",
    "restrict_mass = True\n",
    "use_sigma = True\n",
    "mrich_thresh = 0.1\n",
    "# Go find yourself some fossil groups!\n",
    "for i, this_halo_mask in enumerate(halo_masks):\n",
    "    target_idx = np.nonzero(this_halo_mask)[0]\n",
    "    mainbranch_index, mainbranch_masses = help_func_haccytrees.get_branches(target_idx, forest)\n",
    "    mainbranch_mergers = help_func_haccytrees.get_mainbranch_mergers(forest, progenitor_array, mainbranch_index, absolute_threshold)\n",
    "    major_mergers = help_func_haccytrees.get_major_mergers(mainbranch_mergers, threshold)\n",
    "    lmm_redshifts = help_func_haccytrees.get_lmms(major_mergers, threshold)\n",
    "    fgs, qhs, mrich = help_func_haccytrees.find_specials(forest, mainbranch_index, major_mergers, lmm_redshifts, target_idx, z_thresh, mrich_thresh = mrich_thresh, restrict_mass = restrict_mass, use_sigma = use_sigma, mainbranch_masses = mainbranch_masses)\n",
    "    print(\"bin/; \", str(i))\n",
    "    print(len(target_idx), \" halos\")\n",
    "    print(len(fgs), \" fossils\")\n",
    "    print(len(qhs), \" qhs\")\n",
    "    print(len(mrich), \" merger rich halos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "noble-disabled",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2247323323891456484 2247413818852391226 2247407427941061735 ...\n",
      " 2247442053967417332 2247298236987477760 2247393623916173449]\n",
      "269358\n",
      "2247323323891456484\n"
     ]
    }
   ],
   "source": [
    "print(bin1)\n",
    "print(len(bin1))\n",
    "print(bin1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "mainbranch_index, mainbranch_masses = help_func_haccytrees.get_branches(bin1[0], forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescription-insured",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fossil_groups_env",
   "language": "python",
   "name": "fossil_groups_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
